{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words as nltk_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline , FeatureUnion\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "from time import time\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "DATASET_PATH = 'gender-classifier-DFE-791531.csv'\n",
    "stop = set(stopwords.words('english'))\n",
    "tokenizer = TweetTokenizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>description</th>\n",
       "      <th>link_color</th>\n",
       "      <th>name</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>i sing my own rhythm.</td>\n",
       "      <td>08C2C2</td>\n",
       "      <td>sheezy0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815719227</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>I'm the author of novels filled with family dr...</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>DavdBurnett</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>â°ï¢Ö¿It felt like they were my friends and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815719228</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>louis whining and squealing and all</td>\n",
       "      <td>ABB8C2</td>\n",
       "      <td>lwtprettylaugh</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815719229</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Mobile guy.  49ers, Shazam, Google, Kleiner Pe...</td>\n",
       "      <td>0084B4</td>\n",
       "      <td>douggarland</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815719230</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...</td>\n",
       "      <td>3B94D9</td>\n",
       "      <td>WilfordGemma</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>815719231</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>you don't know me.</td>\n",
       "      <td>F5ABB5</td>\n",
       "      <td>monroevicious</td>\n",
       "      <td>0</td>\n",
       "      <td>Ive seen people on the train with lamps, chair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>815719232</td>\n",
       "      <td>brand</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>A global marketplace for images, videos and mu...</td>\n",
       "      <td>298AAE</td>\n",
       "      <td>Shutterstock</td>\n",
       "      <td>0</td>\n",
       "      <td>@BpackEngineer Thank you for your patience whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>815719233</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>The secret of getting ahead is getting started.</td>\n",
       "      <td>0000FF</td>\n",
       "      <td>RobinMeske</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Gala Bingo clubs bought for ×Â£241m: The UK's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>815719234</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Pll Fan // Crazy about MCD // Ramen is bae</td>\n",
       "      <td>9266CC</td>\n",
       "      <td>pigzilla_</td>\n",
       "      <td>0</td>\n",
       "      <td>@_Aphmau_ the pic defines all mcd fangirls/fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>815719235</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Renaissance art historian, University of Notti...</td>\n",
       "      <td>9266CC</td>\n",
       "      <td>GabrieleNeher</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>@Evielady just how lovely is the tree this yea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  gender  gender:confidence  \\\n",
       "0  815719226    male             1.0000   \n",
       "1  815719227    male             1.0000   \n",
       "2  815719228    male             0.6625   \n",
       "3  815719229    male             1.0000   \n",
       "4  815719230  female             1.0000   \n",
       "5  815719231  female             1.0000   \n",
       "6  815719232   brand             1.0000   \n",
       "7  815719233    male             1.0000   \n",
       "8  815719234  female             1.0000   \n",
       "9  815719235  female             1.0000   \n",
       "\n",
       "                                         description link_color  \\\n",
       "0                              i sing my own rhythm.     08C2C2   \n",
       "1  I'm the author of novels filled with family dr...     0084B4   \n",
       "2                louis whining and squealing and all     ABB8C2   \n",
       "3  Mobile guy.  49ers, Shazam, Google, Kleiner Pe...     0084B4   \n",
       "4  Ricky Wilson The Best FRONTMAN/Kaiser Chiefs T...     3B94D9   \n",
       "5                                 you don't know me.     F5ABB5   \n",
       "6  A global marketplace for images, videos and mu...     298AAE   \n",
       "7    The secret of getting ahead is getting started.     0000FF   \n",
       "8         Pll Fan // Crazy about MCD // Ramen is bae     9266CC   \n",
       "9  Renaissance art historian, University of Notti...     9266CC   \n",
       "\n",
       "             name sidebar_color  \\\n",
       "0         sheezy0        FFFFFF   \n",
       "1     DavdBurnett        C0DEED   \n",
       "2  lwtprettylaugh        C0DEED   \n",
       "3     douggarland        C0DEED   \n",
       "4    WilfordGemma             0   \n",
       "5   monroevicious             0   \n",
       "6    Shutterstock             0   \n",
       "7      RobinMeske        C0DEED   \n",
       "8       pigzilla_             0   \n",
       "9   GabrieleNeher        FFFFFF   \n",
       "\n",
       "                                                text  \n",
       "0  Robbie E Responds To Critics After Win Against...  \n",
       "1  â°ï¢Ö¿It felt like they were my friends and ...  \n",
       "2  i absolutely adore when louis starts the songs...  \n",
       "3  Hi @JordanSpieth - Looking at the url - do you...  \n",
       "4  Watching Neighbours on Sky+ catching up with t...  \n",
       "5  Ive seen people on the train with lamps, chair...  \n",
       "6  @BpackEngineer Thank you for your patience whi...  \n",
       "7  Gala Bingo clubs bought for ×Â£241m: The UK's...  \n",
       "8  @_Aphmau_ the pic defines all mcd fangirls/fan...  \n",
       "9  @Evielady just how lovely is the tree this yea...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_csv(DATASET_PATH, encoding='utf-8',  index_col=False)#sep='\\t' header=None,\n",
    "data = pd.read_csv(DATASET_PATH, encoding='latin1',  index_col=False)#sep='\\t' header=None,\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#filtering out some columns that we will not use:\n",
    "df.drop(['_golden','_unit_state','_trusted_judgments','_last_judgment_at','profile_yn','profile_yn:confidence',\n",
    "        'created','fav_number','gender_gold','profile_yn_gold','profileimage','retweet_count','tweet_coord',\n",
    "        'tweet_count','tweet_created','tweet_id','tweet_location','user_timezone'], axis=1,inplace=True)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix_nan_description(description):\n",
    "    if type(description) is float:\n",
    "        return ''\n",
    "#     if description.encode('latin-1').strip() == 'nan':\n",
    "    if description.strip() == 'nan':\n",
    "        return ''\n",
    "    return description\n",
    "\n",
    "#for example:\n",
    "fix_nan_description('nan ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@ameliaearhart vs @WrightBrothers in an all exclusive fly-off'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "#for example:\n",
    "stem('regardless of context, their affection seemed non-existant')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "#for example:\n",
    "remove_stop_words('regardless of context, their affection seemed non-existant')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokens = [token for token in tokenizer.tokenize(text) if token not in string.punctuation]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def remove_url(text):    \n",
    "    return re.sub(r'https?:\\S+', '', text)\n",
    "\n",
    "#for example:\n",
    "remove_url('for more info on http, enter http://www.http.com (lo beshabat)')\n",
    "\n",
    "def remove_hashtags(text,mode):  \n",
    "    if mode == 'entire_expression':\n",
    "        return re.sub(r'#\\S+', '', text)\n",
    "    elif mode == 'only_symbol':\n",
    "        return text.replace('#','')\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "#for example:\n",
    "remove_hashtags('what did you think about the # of voters this year? use #VotersUnite to let us know.','entire_expression')\n",
    "\n",
    "remove_hashtags('#fridaymadness #nature #fun #outdoor #originalperson','only_symbol')\n",
    "\n",
    "def remove_ats(text,mode):\n",
    "    if mode == 'entire_expression':\n",
    "        return re.sub(r'@\\S+', '', text)\n",
    "    elif mode == 'only_symbol':\n",
    "        return str(text).replace('@','')\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "#for example:\n",
    "remove_ats('@ameliaearhart vs @WrightBrothers in an all exclusive fly-off','entire_expression')\n",
    "\n",
    "remove_ats('@ameliaearhart vs @WrightBrothers in an all exclusive fly-off','only_symbol')\n",
    "\n",
    "def remove_multiple_spaces(text):    \n",
    "    return re.sub(r' +', ' ', text).strip()\n",
    "\n",
    "#for example:\n",
    "remove_multiple_spaces(' @ameliaearhart       vs   @WrightBrothers in an all   exclusive fly-off ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df, min_confidence=0.9, should_lower=False, should_stem=False, should_remove_stop_words=False, should_remove_url=False,\n",
    "                            should_remove_punctuation=False,should_remove_hashtags='none',should_remove_ats='none'):\n",
    "    \n",
    "    #basic cleaning\n",
    "    cleaner_df = df.loc[(df['gender'] != 'nan') & (df['text'] != 'nan') & (df['gender'] != 'unknown')]\n",
    "    \n",
    "    cleaner_df = cleaner_df[df['gender:confidence'] >= min_confidence]\n",
    "            \n",
    "    \n",
    "    \n",
    "    cleaner_df['description'] = [fix_nan_description(desc) for desc in cleaner_df['description']]\n",
    "\n",
    "    \n",
    "    #################################\n",
    "    #removing spam tweets:\n",
    "    cleaner_df.drop_duplicates(subset='text', keep='first', inplace=True)\n",
    "    ##################################\n",
    "    \n",
    "\n",
    "    #todo probably sucks\n",
    "    if should_lower:\n",
    "        cleaner_df['description'] = [desc.lower() for desc in cleaner_df['description']]\n",
    "        cleaner_df['text'] = [text.lower() for text in cleaner_df['text']]\n",
    "\n",
    "    #todo check this (probably sucks so leave it out)\n",
    "    if should_remove_punctuation:\n",
    "        cleaner_df['description'] = [remove_punctuation(desc) for desc in cleaner_df['description']]\n",
    "        cleaner_df['text'] = [remove_punctuation(text) for text in cleaner_df['text']]\n",
    "\n",
    "    if should_remove_url:\n",
    "        cleaner_df['description'] = [remove_url(desc) for desc in cleaner_df['description']]\n",
    "        cleaner_df['text'] = [remove_url(text) for text in cleaner_df['text']]\n",
    "        \n",
    "    if should_remove_hashtags!='none':\n",
    "        cleaner_df['description'] = [remove_hashtags(desc,should_remove_hashtags) for desc in cleaner_df['description']]\n",
    "        cleaner_df['text'] = [remove_hashtags(text,should_remove_hashtags) for text in cleaner_df['text']]\n",
    "        \n",
    "    if should_remove_ats!='none':\n",
    "        cleaner_df['description'] = [remove_ats(desc,should_remove_ats) for desc in cleaner_df['description']]\n",
    "        cleaner_df['text'] = [remove_ats(text,should_remove_ats) for text in cleaner_df['text']]\n",
    "          \n",
    "    if should_remove_stop_words:\n",
    "        cleaner_df['description'] = [remove_stop_words(desc) for desc in cleaner_df['description']]\n",
    "        cleaner_df['text'] = [remove_stop_words(text) for text in cleaner_df['text']]\n",
    "\n",
    "\n",
    "    if should_stem:\n",
    "        cleaner_df['description'] = [stem(desc) for desc in cleaner_df['description']]\n",
    "        cleaner_df['text'] = [stem(text) for text in cleaner_df['text']]\n",
    "        \n",
    "\n",
    "    #remove multiple spaces\n",
    "    cleaner_df['description'] = [remove_multiple_spaces(desc) for desc in cleaner_df['description']]\n",
    "    cleaner_df['text'] = [remove_multiple_spaces(text) for text in cleaner_df['text']]\n",
    "    cleaner_df['text_desc'] = [desc+' '+text for desc,text in zip(cleaner_df['description'],cleaner_df['text'])]\n",
    "\n",
    "    return cleaner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 13165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "clean_df = preprocess(df)\n",
    "print('Number of records: {}'.format(len(clean_df['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-114a594427ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkpt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'theano'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using Theano backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtheano_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mW:\\Anaconda3\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrng_mrg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMRG_RandomStreams\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mRandomStreams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "#same split as with the above model\n",
    "x_train = clean_df['text_desc'][:int(0.9*len(clean_df))]\n",
    "\n",
    "gender_to_int = {'female':0,'male':1,'brand':2}\n",
    "\n",
    "y_train = list([gender_to_int[gender] for gender in clean_df['gender'][:int(0.9*len(clean_df))]]) \n",
    "y_train = keras.utils.to_categorical(y_train, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_WORDS = 10000\n",
    "\n",
    "keras_tokenizer = kpt.Tokenizer(filters='', lower=True,split = ' ',num_words=N_WORDS)#configuring the keras tokenizer to do close to nothing in terms of tokenization\n",
    "\n",
    "texts_for_keras_tokenizer = list([' '.join(tokenizer.tokenize(text)) for text in x_train]) \n",
    "\n",
    "keras_tokenizer.fit_on_texts(texts_for_keras_tokenizer)\n",
    "\n",
    "x_train = keras_tokenizer.texts_to_matrix(texts_for_keras_tokenizer, mode='binary')\n",
    "\n",
    "\n",
    "#todo optimize this\n",
    "model = Sequential()#initialize the NN\n",
    "\n",
    "#add a layer that gets N_WORDS inputs (this is the BOW representation of any tweet) and outputs 512 values ()\n",
    "model.add(Dense(512, input_shape=(N_WORDS,), activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))# this gives a distribution over all 3 genders (probability of the input's belonging to any of the gender)\n",
    "model.compile(loss='categorical_crossentropy',  optimizer='adam',  metrics=['accuracy']) \n",
    "# model.compile(loss='categorical_crossentropy',  optimizer='rmsprop',  metrics=['accuracy'])\n",
    "\n",
    "#we are using the x,y we constructed earlier,\n",
    "model.fit(x_train, y_train,  batch_size=32,  epochs=10,  verbose=1,  validation_split=0.1,  shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = clean_df['text_desc'][int(0.9*len(clean_df)):]\n",
    "\n",
    "y_test = list([gender_to_int[gender] for gender in clean_df['gender'][int(0.9*len(clean_df)):]]) \n",
    "\n",
    "texts_for_keras_tokenizer = list([' '.join(tokenizer.tokenize(text)) for text in x_test]) \n",
    "\n",
    "\n",
    "x_test = keras_tokenizer.texts_to_matrix(texts_for_keras_tokenizer, mode='binary')\n",
    "\n",
    "keras_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "int_to_gender = {0: 'female',1:'male',2:'brand'}\n",
    "\n",
    "actual_pred = list([np.argmax(values) for values in keras_pred])\n",
    "\n",
    "#todo write things\n",
    "print(classification_report(y_test, actual_pred))\n",
    "print('Accuracy score: {}'.format(accuracy_score(y_test, actual_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_combinations(df_training, df_test):\n",
    "    #this produces a list of tuples which are all the possible arrangements of True,False among the cleaning steps we will use\n",
    "    parameter_possibilities = list(itertools.product([0, 1], repeat=7))\n",
    "    \n",
    "    lower_possibility_index = 0\n",
    "    stem_possibility_index = 1\n",
    "    remove_stop_words_possibility_index = 2\n",
    "    remove_url_possibility_index = 3\n",
    "    remove_punctuation_possibility_index = 4\n",
    "    remove_hashtags_possibility_index = 5\n",
    "    remove_ats_possibility_index = 6\n",
    "\n",
    "    #this will be a list of tuples.\n",
    "    res = []\n",
    "    \n",
    "    for possibility in parameter_possibilities:\n",
    "        is_lower = bool(possibility[lower_possibility_index])\n",
    "        is_stemming = bool(possibility[stem_possibility_index])\n",
    "        is_remove_stop_words = bool(possibility[remove_stop_words_possibility_index])\n",
    "        is_remove_url = bool(possibility[remove_url_possibility_index])\n",
    "        is_remove_punctuation = bool(possibility[remove_punctuation_possibility_index])\n",
    "        is_remove_hashtags = bool(possibility[remove_hashtags_possibility_index])\n",
    "        is_remove_hashtags = 'only_symbol' if is_remove_hashtags else 'none'\n",
    "        is_remove_ats = bool(possibility[remove_ats_possibility_index])\n",
    "        is_remove_ats = 'entire_expression' if is_remove_ats else 'none'\n",
    "\n",
    "        \n",
    "        current_training  = preprocess(df_training, \n",
    "                                       should_lower=is_lower,\n",
    "                                       should_stem=is_stemming,\n",
    "                                       should_remove_stop_words=is_remove_stop_words,\n",
    "                                       should_remove_url=is_remove_url,\n",
    "                                       should_remove_punctuation=is_remove_punctuation,\n",
    "                                       should_remove_hashtags = is_remove_hashtags,\n",
    "                                       should_remove_ats = is_remove_ats)\n",
    "        current_test  = preprocess(df_test, \n",
    "                                       should_lower=is_lower,\n",
    "                                       should_stem=is_stemming,\n",
    "                                       should_remove_stop_words=is_remove_stop_words,\n",
    "                                       should_remove_url=is_remove_url,\n",
    "                                       should_remove_punctuation=is_remove_punctuation,\n",
    "                                       should_remove_hashtags = is_remove_hashtags,\n",
    "                                       should_remove_ats = is_remove_ats)\n",
    "\n",
    "        current_tag= '\\nlower : ' + str(is_lower)\n",
    "        current_tag+= '\\nstemming : ' + str(is_stemming)\n",
    "        current_tag+= '\\nremove stop words : '+str(is_remove_stop_words)\n",
    "        current_tag+= '\\nremove urls : '+str(is_remove_url)\n",
    "        current_tag+= '\\nremove punctuation : '+str(is_remove_punctuation)\n",
    "        current_tag+= '\\nremove hashtags : '+str(is_remove_hashtags)\n",
    "        current_tag+= '\\nremove ats : '+str(is_remove_ats)\n",
    "\n",
    "        tagged_clean_dataset = (current_training , current_test , current_tag)\n",
    "        \n",
    "        res.append(tagged_clean_dataset)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = clean_df[:int(0.9*len(clean_df))]#note that the df we use here was cleaned only from spam and irrelevant records. we will now clean it further\n",
    "test_df = clean_df[int(0.9*len(clean_df)):]\n",
    "\n",
    "combinations = preprocessing_combinations(training_df,test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "dl_exact_accuracies = []\n",
    "dl_precisions = []\n",
    "dl_recalls = []\n",
    "dl_f_measures = []\n",
    "N_WORDS = 10000\n",
    "gender_to_int = {'female':0,'male':1,'brand':2}\n",
    "int_to_gender = {0: 'female',1:'male',2:'brand'}\n",
    "for combination in combinations:\n",
    "    x_train_df = combination[0]\n",
    "    x_train = x_train_df['text_desc']\n",
    "    y_train = combination[0]['gender']\n",
    "    y_train = list([gender_to_int[gender] for gender in y_train]) \n",
    "    y_train = keras.utils.to_categorical(y_train, 3)\n",
    "    x_test_df = combination[1]\n",
    "    x_test = x_test_df['text_desc']\n",
    "    y_test = combination[1]['gender']\n",
    "    y_test = list([gender_to_int[gender] for gender in y_test]) \n",
    "#     y_test = keras.utils.to_categorical(y_test, 3)\n",
    "    tag = combination[2]\n",
    "\n",
    "    keras_tokenizer = kpt.Tokenizer(filters='',split = ' ',num_words=N_WORDS)#configuring the keras tokenizer to do close to nothing in terms of tokenization\n",
    "\n",
    "    texts_for_keras_tokenizer = list([' '.join(tokenizer.tokenize(text)) for text in x_train]) \n",
    "\n",
    "    keras_tokenizer.fit_on_texts(texts_for_keras_tokenizer)\n",
    "\n",
    "    x_train = keras_tokenizer.texts_to_matrix(texts_for_keras_tokenizer, mode='binary')\n",
    "\n",
    "\n",
    "    #todo optimize this\n",
    "    model = Sequential()#initialize the NN\n",
    "\n",
    "    #add a layer that gets N_WORDS inputs (this is the BOW representation of any tweet) and outputs 512 values ()\n",
    "    model.add(Dense(512, input_shape=(N_WORDS,), activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))# this gives a distribution over all 3 genders (probability of the input's belonging to any of the gender)\n",
    "    model.compile(loss='categorical_crossentropy',  optimizer='adam',  metrics=['accuracy']) \n",
    "    \n",
    "    \n",
    "    print('Training with combination number '+str(i)+'/'+str(len(combinations)))\n",
    "    \n",
    "    \n",
    "    details = '\\n\\nPreprocessing parameters:\\n'+tag\n",
    "    \n",
    "\n",
    "    t0 = time()\n",
    "    model.fit(x_train, y_train,  batch_size=32,  epochs=7,  verbose=1,  validation_split=0.1,  shuffle=True)\n",
    "    train_time = time() - t0\n",
    "    details+=\"\\nFeature extraction + training time: %0.3fs\" % train_time\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "#     pred = baseline.predict(x_test)\n",
    "    texts_for_keras_tokenizer = list([' '.join(tokenizer.tokenize(text)) for text in x_test]) \n",
    "    x_test = keras_tokenizer.texts_to_matrix(texts_for_keras_tokenizer, mode='binary')\n",
    "\n",
    "    keras_pred = model.predict(x_test)\n",
    "\n",
    "    actual_pred = list([np.argmax(values) for values in keras_pred])\n",
    "\n",
    "    test_time = time() - t0\n",
    "    details+=\"\\nTest time:  %0.3fs\" % test_time\n",
    "\n",
    "    exact_accuracy = metrics.accuracy_score(y_test, actual_pred)\n",
    "    details+=\"\\n\\nAccuracy:   %0.3f\" % exact_accuracy\n",
    "\n",
    "    details+='\\nMicro-averaged metrics: '\n",
    "\n",
    "    precision_micro = metrics.precision_score(y_test, actual_pred, average='micro')\n",
    "    details+=\"\\nPrecision:   %0.3f\" % precision_micro\n",
    "\n",
    "    recall_micro = metrics.recall_score(y_test, actual_pred, average='micro')\n",
    "    details+=\"\\nRecall:   %0.3f\" % recall_micro\n",
    "\n",
    "    f_measure_micro = metrics.f1_score(y_test, actual_pred, average='micro')                              \n",
    "    details+=\"\\nF_measure:   %0.3f\" % f_measure_micro\n",
    "\n",
    "    details+='\\nMacro-averaged metrics: '\n",
    "\n",
    "    precision_macro = metrics.precision_score(y_test, actual_pred, average='macro')\n",
    "    details+=\"\\nPrecision:   %0.3f\" % precision_macro\n",
    "\n",
    "    recall_macro = metrics.recall_score(y_test, actual_pred, average='macro')\n",
    "    details+=\"\\nRecall:   %0.3f\" % recall_macro\n",
    "\n",
    "    f_measure_macro = metrics.f1_score(y_test, actual_pred, average='macro')                              \n",
    "    details+=\"\\nF_measure:   %0.3f\" % f_measure_macro\n",
    "\n",
    "    details+='\\nWeighted-average metrics: '\n",
    "\n",
    "    precision_weighted = metrics.precision_score(y_test, actual_pred, average='weighted')\n",
    "    details+=\"\\nPrecision:   %0.3f\" % precision_weighted\n",
    "\n",
    "    recall_weighted = metrics.recall_score(y_test, actual_pred, average='weighted')\n",
    "    details+=\"\\nRecall:   %0.3f\" % recall_weighted\n",
    "\n",
    "    f_measure_weighted = metrics.f1_score(y_test, actual_pred, average='weighted')                              \n",
    "    details+=\"\\nF_measure:   %0.3f\" % f_measure_weighted\n",
    "        \n",
    "        \n",
    "    dl_exact_accuracies.append((exact_accuracy,details))\n",
    "    dl_precisions.append((max(precision_micro,precision_macro,precision_weighted),details))    \n",
    "    dl_recalls.append((max(recall_micro,recall_macro,recall_weighted),details))\n",
    "    dl_f_measures.append((max(f_measure_micro,f_measure_macro,f_measure_weighted),details))\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_accuracies = sorted(dl_exact_accuracies, key=lambda x: x[0])[-10:]\n",
    "sorted_precisions = sorted(dl_precisions, key=lambda x: x[0])[-10:]\n",
    "sorted_recalls = sorted(dl_recalls, key=lambda x: x[0])[-10:]\n",
    "sorted_f_measures = sorted(dl_f_measures, key=lambda x: x[0])[-10:]\n",
    "\n",
    "print('-'*8 + 'Top 10 accuracies: '+'-'*8)\n",
    "for value,details in sorted_accuracies:\n",
    "    print(details)\n",
    "   \n",
    "print('-'*8 + 'Top 10 precisions: '+'-'*8)\n",
    "for value,details in sorted_precisions:\n",
    "    print(details)  \n",
    "\n",
    "print('-'*8 + 'Top 10 recalls: '+'-'*8)\n",
    "for value,details in sorted_recalls:\n",
    "    print(details)   \n",
    "\n",
    "print('-'*8 + 'Top 10 f-measures: '+'-'*8)\n",
    "for value,details in sorted_f_measures:\n",
    "    print(details) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
